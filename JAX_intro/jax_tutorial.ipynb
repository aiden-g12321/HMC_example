{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f0961d",
   "metadata": {},
   "source": [
    "# JAX / NumPyro tutorial\n",
    "\n",
    "- What is JAX?\n",
    "\n",
    "- JAX basics\n",
    "\n",
    "- Things no one tells you\n",
    "\n",
    "- Sampling using NumPyro HMC and JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6537818",
   "metadata": {},
   "source": [
    "### NVIDIA stock\n",
    "<img src=\"nvidia.png\" alt=\"\" style=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa79048",
   "metadata": {},
   "source": [
    "## What is JAX?\n",
    "\n",
    "JAX is a Python library for array-oriented numerical computation.\n",
    "\n",
    "- NumPy-like interface which runs on CPU, GPU, or TPU.\n",
    "- Features Just-In-Time (JIT) compilation\n",
    "- Automatic vectorization.\n",
    "- Automatic differentiation of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f563efe",
   "metadata": {},
   "source": [
    "## JAX basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51420997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you've never used JAX or NumPyro.\n",
    "\n",
    "# !pip install numpyro\n",
    "# !pip install jax\n",
    "# !pip install corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy as js\n",
    "import jax.random as jr\n",
    "\n",
    "# for comparison\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# for sampling later\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "numpyro.set_host_device_count(10)\n",
    "\n",
    "# plotting packages\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f090a074",
   "metadata": {},
   "source": [
    "### Single-precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default JAX uses single-precision (float32)\n",
    "# jax.config.update('jax_enable_x64', True)\n",
    "print(np.cos(np.pi / 3))\n",
    "print(jnp.cos(jnp.pi / 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1992b",
   "metadata": {},
   "source": [
    "### NumPy-like arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08847f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = np.array([4.3, -7.3, 0.001])\n",
    "jax_array = jnp.array([4.3, -7.3, 0.001])\n",
    "print(type(numpy_array))\n",
    "print(type(jax_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12287e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(np.array(jax_array)))\n",
    "print(type(jnp.array(numpy_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = jnp.array([1., -2.5, 0.6])\n",
    "array2 = jnp.array([5.3, 5.002, -11.5])\n",
    "array1 + array2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c705a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.inner(array1, array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb7f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.outer(array1, array2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.linalg.eig(jnp.outer(array1, array2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c314497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other common arrays\n",
    "a = jnp.zeros((3, 3))       # 2D array of zeros\n",
    "b = jnp.ones((3, 3, 5))     # 3D array of ones\n",
    "c = jnp.arange(0, 10, 2)    # [0, 2, 4, 6, 8]\n",
    "d = jnp.linspace(0, 1, 5)   # [0., 0.25, 0.5, 0.75, 1.]\n",
    "e = jnp.eye(4)              # 4x4 identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array slicing\n",
    "b[3, 1]\n",
    "c[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a155b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# broadcasting\n",
    "f = (a[..., None] * b)[..., None] / jnp.outer(c, d)\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.random uses PRNG keys\n",
    "random_seed = 150914\n",
    "a_PRNG_key = jr.key(random_seed)\n",
    "random_numbers = jr.normal(key=a_PRNG_key, shape=(100, 100))\n",
    "\n",
    "# need many PRNG keys?\n",
    "many_PRNG_keys = jr.split(key=a_PRNG_key, num=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c32bc9",
   "metadata": {},
   "source": [
    "### Some things are different..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a547aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX arrays are expensive to create!!!\n",
    "%timeit np.array([5, 5, 5])\n",
    "%timeit jnp.array([5, 5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42692ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you modify JAX arrays? No!\n",
    "\n",
    "numpy_array = np.array([0, 1, 2, 3, 4, 5])\n",
    "jax_array = jnp.array([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "numpy_array[3] = 0\n",
    "print(numpy_array)\n",
    "\n",
    "# jax_array[3] = 0    # error!\n",
    "# # jax_array = jax_array.at[3].set(0)\n",
    "# print(jax_array)\n",
    "\n",
    "# # also try...\n",
    "# numpy_array[2] += 1\n",
    "# # jax_array[2] += 1   # error!\n",
    "# jax_array = jax_array.at[2].add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit numpy_array[3] = 0\n",
    "%timeit jax_array.at[3].set(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63051093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we speed up JAX array modification?\n",
    "arr = jnp.arange(10, dtype=jnp.float32)\n",
    "zeros = jnp.zeros(5, dtype=jnp.float32)\n",
    "%timeit arr.at[-5:10].set(0.)\n",
    "%timeit jnp.concatenate((arr[:-5], zeros), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60705c6a",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "\n",
    "- JAX arrays operate similar to NumPy arrays.\n",
    "- JAX arrays are expensive to create and modify!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce61bcfd",
   "metadata": {},
   "source": [
    "## Just-In-Time (JIT) compilation\n",
    "\n",
    "Suppose we have the function $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$,\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(x, y) = \\text{Tr}\\big[(\\mathbf{A} + x\\mathbf{I})^\\text{T}\\,(\\mathbf{A} + y\\mathbf{I})^3\\big]\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mathbf{A}$ is a constant matrix and $\\mathbf{I}$ is the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7644411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_func(x, y, A):\n",
    "    M = (A + x * np.eye(A.shape[0])).T @ (A + y * np.eye(A.shape[0]))**3\n",
    "    return np.trace(M)\n",
    "\n",
    "def jax_func(x, y, A):\n",
    "    M = (A + x * jnp.eye(A.shape[0])).T @ (A + y * jnp.eye(A.shape[0]))**3\n",
    "    return jnp.trace(M)\n",
    "\n",
    "# some inputs\n",
    "x = 2.5\n",
    "y = -3.9\n",
    "jax_A = jr.normal(jr.key(200129), (100, 100))\n",
    "numpy_A = np.array(jax_A)\n",
    "\n",
    "print(numpy_func(x, y, numpy_A))\n",
    "print(jax_func(x, y, jax_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c0751",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit numpy_func(x, y, numpy_A)\n",
    "%timeit jax_func(x, y, jax_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fedd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JIT JAX likelihood\n",
    "fast_jax_func = jax.jit(jax_func)\n",
    "\n",
    "# test\n",
    "print(fast_jax_func(x, y, jax_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit fast_jax_func(x, y, jax_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c0ab04",
   "metadata": {},
   "source": [
    "### Can you JIT everything? No..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9eee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return True if even, False if odd\n",
    "def is_even(integer):\n",
    "    if integer % 2 == 0:  # even\n",
    "        return True\n",
    "    else:  # odd\n",
    "        return False\n",
    "    \n",
    "fast_is_even = jax.jit(is_even)\n",
    "\n",
    "# print(is_even(6))\n",
    "print(fast_is_even(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditionals work-around\n",
    "def spit_True():\n",
    "    return True\n",
    "\n",
    "def spit_False():\n",
    "    return False\n",
    "\n",
    "def is_even_redo(integer):\n",
    "    even = (integer % 2 == 0)\n",
    "    return jax.lax.cond(even, spit_True, spit_False)\n",
    "\n",
    "fast_is_even_redo = jax.jit(is_even_redo)\n",
    "print(fast_is_even_redo(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit is_even(6)\n",
    "%timeit fast_is_even_redo(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3edf008",
   "metadata": {},
   "source": [
    "#### Takeaways:\n",
    "\n",
    "- JIT compile expensive functions which are called many times (e.g. likelihood evaluations)\n",
    "- You can't JIT compile anything..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc5f19",
   "metadata": {},
   "source": [
    "## Automatic vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7050076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose we want to evaluate the function over many inputs\n",
    "num_evaluations = 100\n",
    "many_x = jr.normal(jr.key(230814), (num_evaluations,))\n",
    "many_y = jr.normal(jr.key(250114), (num_evaluations,))\n",
    "jax_A = jr.normal(jr.key(170817), (10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ecf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit [fast_jax_func(x, y, jax_A) for x, y in zip(many_x, many_y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d6f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can automatically vectorize JAX functions\n",
    "vectorized_jax_func = jax.jit(jax.vmap(fast_jax_func, in_axes=(0, 0, None)))\n",
    "\n",
    "# test\n",
    "print(vectorized_jax_func(many_x, many_y, jax_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0327be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit vectorized_jax_func(many_x, many_y, jax_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc7599",
   "metadata": {},
   "source": [
    "## Automatic differentiation\n",
    "\n",
    "Suppose $f(x) = x\\cos(3x),\\hspace{1.5mm} g(x) = e^{x^2}\\,\\ln(x), \\hspace{1.5mm}\\text{and}\\hspace{1.5mm} h(x) = \\cos(x)e^{\\tan(x)}$. The $\\textit{analytic}$ derivative of the product,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\frac{d}{dx}\\bigg[f(x)\\,g(x)\\,h(x)\\bigg] = f'\\, g\\, h + f\\, g'\\, h + f\\, g\\, h'\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "grows geometrically due to the product and chain rule.\n",
    "\n",
    "Numerical derivatives, like finite differencing, are computationally expensive in high-dimensions, and must be tuned to be numerically stable.\n",
    "\n",
    "Automatic differentiation computes derivatives with directed acyclic graphs (DAGs). This is $\\textit{fast}$ and $\\textit{exact}$, even for high-dimensional functions. Consider the example function,\n",
    "\n",
    "\\begin{equation*}\n",
    "    f(x_1, x_2) = x_1 \\,\\text{exp}\\bigg[-\\frac{1}{2}(x_1^2 + x_2^2)\\bigg]\\,.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983a507",
   "metadata": {},
   "source": [
    "<img src=\"graph.png\" alt=\"Negative\" style=\"filter: invert(1);\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d334a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can take derivatives of JAX functions\n",
    "grad_jax_func = jax.jit(jax.grad(fast_jax_func, argnums=(0, 1)))\n",
    "\n",
    "# test\n",
    "print(grad_jax_func(x, y, jax_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f35f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit fast_jax_func(x, y, jax_A)\n",
    "%timeit grad_jax_func(x, y, jax_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try these\n",
    "# jax.hessian\n",
    "# jax.jacobian\n",
    "# jax.jit(jax.vmap(jax.hessian(...)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ad4e26",
   "metadata": {},
   "source": [
    "## Best practices\n",
    "\n",
    "- Initialize and store as many jax.arrays as possible at the start of your code. Avoid creating/modifying jax.arrays in code which is executed many times.\n",
    "\n",
    "- JIT compile functions.\n",
    "\n",
    "- Compose auto-diff, auto-vec, and JIT wrappers.\n",
    "\n",
    "- Break up your code into JAX blocks, and non-JAX blocks. Use JAX for heavy repeated computations (e.g. likelihood evaluations), NumPy for bookkeeping (e.g. modifying chains).\n",
    "\n",
    "- Itâ€™s easier to start a project in JAX from the beginning than to add it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da26a4",
   "metadata": {},
   "source": [
    "## Sampling with HMC in NumPyro\n",
    "\n",
    "For a target density, $\\pi(\\mathbf{q})$, from which we want to sample, Hamiltonian Monte Carlo (HMC) defines a Hamiltonian,\n",
    "\n",
    "\\begin{align*}\n",
    "    H(\\mathbf{q}, \\mathbf{p}) &= T(\\mathbf{p}) + V(\\mathbf{q}) \\\\\n",
    "    &= T(\\mathbf{p}) - \\ln\\pi(\\mathbf{q})\\,.\n",
    "\\end{align*}\n",
    "\n",
    "If we start from a sample $\\mathbf{q}_0$ (and randomized initial momentum $\\mathbf{p}_0$), subsequent samples are proposed by integrating Hamilton's equations,\n",
    "\n",
    "\\begin{align*}\n",
    "    \\dot{q}_i = &\\frac{\\partial H}{\\partial p_i} \\\\\n",
    "    \\dot{p}_i = -&\\frac{\\partial H}{\\partial q_i}\n",
    "\\end{align*}\n",
    "\n",
    "for some period of time. After integrating for some time to points $(\\mathbf{q}_\\text{final}, \\mathbf{p}_\\text{final})$, the acceptance probability is,\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\alpha = \\text{min}\\bigg(1,\\,\\frac{\\text{exp}[-H(\\mathbf{q}_\\text{final}, \\mathbf{p}_\\text{final})]}{\\text{exp}[-H(\\mathbf{q}_0, \\mathbf{p}_0)]}\\bigg)\\,.\n",
    "\\end{equation*}\n",
    "\n",
    "#### Pros\n",
    "- Scales very well with high-dimension (easily samples 1000s of dimensions)\n",
    "- Finds a (local) peak very quickly\n",
    "- Long jump proposals, i.e. low auto-correlation in chain\n",
    "\n",
    "#### Cons\n",
    "- Need partial derivatives of target density (not a con if you're using JAX)\n",
    "- Struggles with multi-modal distributions\n",
    "- Difficult to mix in other proposals in current implementations\n",
    "\n",
    "\n",
    "Animation of HMC: https://chi-feng.github.io/mcmc-demo/app.html\n",
    "\n",
    "Start by defining a likelihood in JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update('jax_enable_x64', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f372ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_unit = 1.\n",
    "def ring_lnlike(x, y):\n",
    "    r = jnp.sqrt(x**2 + y**2)\n",
    "    return -5. * (r - r_unit)**2\n",
    "\n",
    "vectorized_ring_lnlike = jax.vmap(ring_lnlike, in_axes=(0, 0))\n",
    "\n",
    "def mobius_ladder_lnlike(xs, coupling_weights):\n",
    "    n = xs.shape[0]\n",
    "\n",
    "    # rails of ladder\n",
    "    rail_terms = vectorized_ring_lnlike(xs, jnp.roll(xs, 1)) + vectorized_ring_lnlike(xs, jnp.roll(xs, -1))\n",
    "\n",
    "    # rungs of ladder\n",
    "    rung_terms = vectorized_ring_lnlike(xs, jnp.roll(xs, n // 2))\n",
    "    \n",
    "    total_lnlike_val = jnp.sum((rail_terms - 0.1 * rung_terms) * coupling_weights)\n",
    "    return total_lnlike_val\n",
    "\n",
    "fast_mobius_ladder_lnlike = jax.jit(mobius_ladder_lnlike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272d79ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# application of auto-differentiation\n",
    "example_x_input = jr.normal(jr.key(190521), (1000,))\n",
    "example_coupling_weights = jnp.ones_like(example_x_input)\n",
    "\n",
    "grad_mobius_ladder_lnlike = jax.jit(jax.grad(fast_mobius_ladder_lnlike, argnums=(0)))\n",
    "print(grad_mobius_ladder_lnlike(example_x_input, example_coupling_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit fast_mobius_ladder_lnlike(example_x_input, example_coupling_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c9fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit grad_mobius_ladder_lnlike(example_x_input, example_coupling_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95944eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_negative_Fisher = jax.jit(jax.hessian(fast_mobius_ladder_lnlike, argnums=(0)))\n",
    "print(get_negative_Fisher(example_x_input, example_coupling_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c535a3f",
   "metadata": {},
   "source": [
    "A fun hierarchical model,\n",
    "\n",
    "\\begin{equation*}\n",
    "    p(\\mathbf{x}, \\sigma, k | d) \\propto p(d | \\mathbf{x}) \\cdot p(\\mathbf{x} | \\sigma) \\cdot p(\\sigma | k) \\cdot p(k)\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{align*}\n",
    "    d | \\mathbf{x} \\,\\sim & \\,\\text{``sum of rings (whose covariance is topologically a M\\\"{o}bius ladder)\"} \\\\\n",
    "    \\mathbf{x} | \\sigma \\,\\sim & \\,\\mathcal{N}(0, \\sigma^2) \\\\\n",
    "    \\sigma | k \\,\\sim & \\,\\chi^2(k) \\\\\n",
    "    k \\,\\sim & \\,\\text{Uniform(0, 100)}\\,. \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10301e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPyro sampling model\n",
    "def model(ndim, coupling_weights):\n",
    "    \n",
    "    # hyper-hyper-prior\n",
    "    k = numpyro.sample('k', dist.Uniform(0., 100.))\n",
    "\n",
    "    # hyper-prior\n",
    "    sigma = numpyro.sample('sigma', dist.Chi2(k))\n",
    "\n",
    "    # prior\n",
    "    x = numpyro.sample('x', dist.Normal(0., sigma).expand((ndim,)))\n",
    "\n",
    "    # likelihood\n",
    "    numpyro.factor('lnlike', fast_mobius_ladder_lnlike(x, coupling_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define No U-Turn Sampling kernel\n",
    "nuts_kernel = numpyro.infer.NUTS(model=model)\n",
    "\n",
    "# set-up MCMC\n",
    "mcmc = numpyro.infer.MCMC(sampler=nuts_kernel,\n",
    "                          num_warmup=int(5e3),\n",
    "                          num_samples=int(1e5),\n",
    "                          num_chains=10)\n",
    "\n",
    "# run MCMC\n",
    "ndim = 8\n",
    "# ndim = 100  # try this...\n",
    "coupling_weights = 3.5 * jnp.sin(jnp.pi * np.arange(ndim) / ndim)\n",
    "mcmc.run(jr.key(170817), ndim, coupling_weights)\n",
    "\n",
    "# save chain\n",
    "samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e56a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution on low-level parameters\n",
    "x_samples = np.array(samples['x'])\n",
    "x_labels = np.array([rf'$x_{{{i}}}$' for i in range(1, ndim + 1)])\n",
    "fig = corner.corner(x_samples,\n",
    "                    labels=x_labels,\n",
    "                    bins=40,\n",
    "                    label_kwargs={'fontsize': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d976a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot distribution on hyper-parameters\n",
    "# fig = corner.corner(np.array([samples['k'], samples['sigma']]).T,\n",
    "#                     labels=[r'$k$', r'$\\sigma$'],\n",
    "#                     bins=40,\n",
    "#                     range=[0.99]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9cd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
